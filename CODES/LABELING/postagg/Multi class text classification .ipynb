{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be imported\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "1da17257-6c48-42fe-b761-15148be8a56a",
    "_uuid": "ddb47654b861d29d92251edeed58352f05490e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['fasilitas', 'lokasi', 'serbaneka', 'suasana'], dtype=object), array([ 5698, 11027, 22220,  6792], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Read the input dataset \n",
    "d = pd.read_csv(\"../tokenizedsenakhir.csv\", \n",
    "                usecols=('classified_Aspek','Review_bersih'),\n",
    "                dtype={'Review': object})\n",
    "# Only interested in data with consumer complaints\n",
    "d=d[d['Review_bersih'].notnull()]\n",
    "\n",
    "d=d[d['classified_Aspek'].notnull()]\n",
    "d.reset_index(drop=True,inplace=True)\n",
    "x = d.iloc[:, 0].values\n",
    "y = d.iloc[:, 1].values\n",
    "# print(x)\n",
    "\n",
    "#there are 11 unique classes for classification\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    4  152 5859]\n",
      " [   0    0    0 ... 1305  595    9]\n",
      " [   0    0    0 ...  238  344  283]\n",
      " ...\n",
      " [   0    0    0 ...    4  670 3960]\n",
      " [   0    0    0 ... 2688    7  896]\n",
      " [   0    0    0 ...  174   93  324]]\n"
     ]
    }
   ],
   "source": [
    " # encode the text with word sequences - Preprocessing step 1\n",
    "tk = Tokenizer(num_words= 8000, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=\" \")\n",
    "tk.fit_on_texts(x)\n",
    "x = tk.texts_to_sequences(x)\n",
    "x = sequence.pad_sequences(x, maxlen=1500)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45737, 1500)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 2 1 2]\n",
      "(array([0, 1, 2, 3]), array([ 5698, 11027, 22220,  6792], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# # Label Encoding categorical data for the classification category\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# labelencoder_Y = LabelEncoder()\n",
    "# y = labelencoder_Y.fit_transform(y)\n",
    "# print(y)\n",
    "# print(np.unique(y, return_counts=True))\n",
    "\n",
    "# Label Encoding categorical data for the classification category\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_Y = LabelEncoder()\n",
    "y = labelencoder_Y.fit_transform(y)\n",
    "print(y)\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Perform one hot encoding \n",
    "from tensorflow.keras import utils as np_utils\n",
    "y = np_utils.to_categorical(y, num_classes= 4)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding\n",
    "np.random.seed(200)\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_from=3\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        x = [[start_char] + [w + index_from for w in x1] for x1 in x]\n",
    "elif index_from:\n",
    "        x = [[w + index_from for w in x1] for x1 in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in x])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        x = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in x]\n",
    "else:\n",
    "        x = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in x]\n",
    "        \n",
    "# split test and train data\n",
    "test_split = 0.2\n",
    "idx = int(len(x) * (1 - test_split))\n",
    "x_train, y_train = np.array(x[:idx]), np.array(y[:idx])\n",
    "x_test, y_test = np.array(x[idx:]), np.array(y[idx:])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (33004, 53)\n",
      "x_test shape: (8251, 53)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=53)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=53)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test shape: (8251, 4)\n"
     ]
    }
   ],
   "source": [
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train on 33004 samples, validate on 8251 samples\n",
      "Epoch 1/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 1.0126 - acc: 0.5610 - val_loss: 0.7890 - val_acc: 0.6728\n",
      "Epoch 2/50\n",
      "33004/33004 [==============================] - 53s 2ms/sample - loss: 0.5743 - acc: 0.7773 - val_loss: 0.4908 - val_acc: 0.8029\n",
      "Epoch 3/50\n",
      "33004/33004 [==============================] - 56s 2ms/sample - loss: 0.4368 - acc: 0.8245 - val_loss: 0.4623 - val_acc: 0.8171\n",
      "Epoch 4/50\n",
      "33004/33004 [==============================] - 55s 2ms/sample - loss: 0.3910 - acc: 0.8438 - val_loss: 0.4185 - val_acc: 0.8317\n",
      "Epoch 5/50\n",
      "33004/33004 [==============================] - 55s 2ms/sample - loss: 0.3658 - acc: 0.8531 - val_loss: 0.4005 - val_acc: 0.8401\n",
      "Epoch 6/50\n",
      "33004/33004 [==============================] - 52s 2ms/sample - loss: 0.3474 - acc: 0.8613 - val_loss: 0.4255 - val_acc: 0.8330\n",
      "Epoch 7/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 0.3317 - acc: 0.8688 - val_loss: 0.4368 - val_acc: 0.8326\n",
      "Epoch 8/50\n",
      "33004/33004 [==============================] - 55s 2ms/sample - loss: 0.3181 - acc: 0.8746 - val_loss: 0.4081 - val_acc: 0.8429\n",
      "Epoch 9/50\n",
      "33004/33004 [==============================] - 52s 2ms/sample - loss: 0.3081 - acc: 0.8778 - val_loss: 0.4104 - val_acc: 0.8430\n",
      "Epoch 10/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 0.2970 - acc: 0.8838 - val_loss: 0.4049 - val_acc: 0.8435\n",
      "Epoch 11/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 0.2863 - acc: 0.8879 - val_loss: 0.4122 - val_acc: 0.8447\n",
      "Epoch 12/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 0.2722 - acc: 0.8940 - val_loss: 0.4422 - val_acc: 0.8390\n",
      "Epoch 13/50\n",
      "33004/33004 [==============================] - 57s 2ms/sample - loss: 0.2609 - acc: 0.8989 - val_loss: 0.4405 - val_acc: 0.8389\n",
      "Epoch 14/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 0.2465 - acc: 0.9059 - val_loss: 0.4640 - val_acc: 0.8335\n",
      "Epoch 15/50\n",
      "33004/33004 [==============================] - 53s 2ms/sample - loss: 0.2424 - acc: 0.9068 - val_loss: 0.4719 - val_acc: 0.8358\n",
      "Epoch 16/50\n",
      "33004/33004 [==============================] - 57s 2ms/sample - loss: 0.2300 - acc: 0.9109 - val_loss: 0.4675 - val_acc: 0.8330\n",
      "Epoch 17/50\n",
      "33004/33004 [==============================] - 57s 2ms/sample - loss: 0.2211 - acc: 0.9161 - val_loss: 0.4993 - val_acc: 0.8323\n",
      "Epoch 18/50\n",
      "33004/33004 [==============================] - 56s 2ms/sample - loss: 0.2132 - acc: 0.9162 - val_loss: 0.4859 - val_acc: 0.8360\n",
      "Epoch 19/50\n",
      "33004/33004 [==============================] - 53s 2ms/sample - loss: 0.2013 - acc: 0.9227 - val_loss: 0.5122 - val_acc: 0.8323\n",
      "Epoch 20/50\n",
      "33004/33004 [==============================] - 54s 2ms/sample - loss: 0.1932 - acc: 0.9255 - val_loss: 0.5091 - val_acc: 0.8360\n",
      "Epoch 21/50\n",
      "33004/33004 [==============================] - 58s 2ms/sample - loss: 0.1826 - acc: 0.9301 - val_loss: 0.5399 - val_acc: 0.8370\n",
      "Epoch 22/50\n",
      "33004/33004 [==============================] - 56s 2ms/sample - loss: 0.1771 - acc: 0.9318 - val_loss: 0.5689 - val_acc: 0.8324\n",
      "Epoch 23/50\n",
      "33004/33004 [==============================] - 56s 2ms/sample - loss: 0.1707 - acc: 0.9360 - val_loss: 0.5457 - val_acc: 0.8363\n",
      "Epoch 24/50\n",
      "33004/33004 [==============================] - 58s 2ms/sample - loss: 0.1640 - acc: 0.9389 - val_loss: 0.5809 - val_acc: 0.8334\n",
      "Epoch 25/50\n",
      "33004/33004 [==============================] - 57s 2ms/sample - loss: 0.1603 - acc: 0.9392 - val_loss: 0.5923 - val_acc: 0.8295\n",
      "Epoch 26/50\n",
      "33004/33004 [==============================] - 57s 2ms/sample - loss: 0.1561 - acc: 0.9415 - val_loss: 0.5908 - val_acc: 0.8315\n",
      "Epoch 27/50\n",
      "33004/33004 [==============================] - 52s 2ms/sample - loss: 0.1510 - acc: 0.9440 - val_loss: 0.6073 - val_acc: 0.8325\n",
      "Epoch 28/50\n",
      "33004/33004 [==============================] - 61s 2ms/sample - loss: 0.1483 - acc: 0.9425 - val_loss: 0.6276 - val_acc: 0.8267\n",
      "Epoch 29/50\n",
      "33004/33004 [==============================] - 57s 2ms/sample - loss: 0.1417 - acc: 0.9473 - val_loss: 0.6373 - val_acc: 0.8302\n",
      "Epoch 30/50\n",
      "33004/33004 [==============================] - 66s 2ms/sample - loss: 0.1353 - acc: 0.9491 - val_loss: 0.6595 - val_acc: 0.8336\n",
      "Epoch 31/50\n",
      "33004/33004 [==============================] - 64s 2ms/sample - loss: 0.1328 - acc: 0.9505 - val_loss: 0.6771 - val_acc: 0.8254\n",
      "Epoch 32/50\n",
      "33004/33004 [==============================] - 66s 2ms/sample - loss: 0.1305 - acc: 0.9506 - val_loss: 0.6628 - val_acc: 0.8295\n",
      "Epoch 33/50\n",
      "33004/33004 [==============================] - 60s 2ms/sample - loss: 0.1214 - acc: 0.9543 - val_loss: 0.7259 - val_acc: 0.8220\n",
      "Epoch 34/50\n",
      "33004/33004 [==============================] - 58s 2ms/sample - loss: 0.1223 - acc: 0.9546 - val_loss: 0.6926 - val_acc: 0.8283\n",
      "Epoch 35/50\n",
      "33004/33004 [==============================] - 64s 2ms/sample - loss: 0.1181 - acc: 0.9553 - val_loss: 0.7164 - val_acc: 0.8311\n",
      "Epoch 36/50\n",
      "33004/33004 [==============================] - 72s 2ms/sample - loss: 0.1143 - acc: 0.9582 - val_loss: 0.7185 - val_acc: 0.8312\n",
      "Epoch 37/50\n",
      "33004/33004 [==============================] - 64s 2ms/sample - loss: 0.1140 - acc: 0.9582 - val_loss: 0.7169 - val_acc: 0.8311\n",
      "Epoch 38/50\n",
      "33004/33004 [==============================] - 67s 2ms/sample - loss: 0.1146 - acc: 0.9576 - val_loss: 0.7290 - val_acc: 0.8277\n",
      "Epoch 39/50\n",
      "33004/33004 [==============================] - 65s 2ms/sample - loss: 0.1068 - acc: 0.9604 - val_loss: 0.7909 - val_acc: 0.8249\n",
      "Epoch 40/50\n",
      "33004/33004 [==============================] - 65s 2ms/sample - loss: 0.1088 - acc: 0.9594 - val_loss: 0.7497 - val_acc: 0.8284\n",
      "Epoch 41/50\n",
      "33004/33004 [==============================] - 66s 2ms/sample - loss: 0.1023 - acc: 0.9622 - val_loss: 0.7938 - val_acc: 0.8209\n",
      "Epoch 42/50\n",
      "33004/33004 [==============================] - 65s 2ms/sample - loss: 0.1011 - acc: 0.9618 - val_loss: 0.7963 - val_acc: 0.8284\n",
      "Epoch 43/50\n",
      "33004/33004 [==============================] - 61s 2ms/sample - loss: 0.0996 - acc: 0.9634 - val_loss: 0.8100 - val_acc: 0.8241\n",
      "Epoch 44/50\n",
      "33004/33004 [==============================] - 63s 2ms/sample - loss: 0.1020 - acc: 0.9620 - val_loss: 0.8133 - val_acc: 0.8254\n",
      "Epoch 45/50\n",
      "33004/33004 [==============================] - 51s 2ms/sample - loss: 0.0990 - acc: 0.9640 - val_loss: 0.8234 - val_acc: 0.8256\n",
      "Epoch 46/50\n",
      "33004/33004 [==============================] - 43s 1ms/sample - loss: 0.0927 - acc: 0.9655 - val_loss: 0.8357 - val_acc: 0.8256\n",
      "Epoch 47/50\n",
      "33004/33004 [==============================] - 38s 1ms/sample - loss: 0.0938 - acc: 0.9661 - val_loss: 0.8634 - val_acc: 0.8241\n",
      "Epoch 48/50\n",
      "33004/33004 [==============================] - 38s 1ms/sample - loss: 0.0884 - acc: 0.9678 - val_loss: 0.8661 - val_acc: 0.8234\n",
      "Epoch 49/50\n",
      "33004/33004 [==============================] - 39s 1ms/sample - loss: 0.0884 - acc: 0.9669 - val_loss: 0.8760 - val_acc: 0.8215\n",
      "Epoch 50/50\n",
      "33004/33004 [==============================] - 40s 1ms/sample - loss: 0.0867 - acc: 0.9677 - val_loss: 0.8583 - val_acc: 0.8205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ba1cb13b00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 2000\n",
    "maxlen = 53\n",
    "embedding_dims = 50\n",
    "filters = 64\n",
    "kernel_size = 20\n",
    "hidden_dims = 250\n",
    "\n",
    "\n",
    "# CNN with max pooling imeplementation \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='sigmoid',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = model.evaluate(x_train, y_train, verbose=1)\n",
    "print(\"Accuracy: %.2f\" % (acc*100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f\" % (acc*100), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
